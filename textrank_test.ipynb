{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:2000px;  /* your desired max-height here */\n",
       "}\n",
       ".output_scroll {\n",
       "    box-shadow:none !important;\n",
       "    webkit-box-shadow:none !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:2000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import math\n",
    "\n",
    "\n",
    "# added english stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "MULTIPLE_WHITESPACE_PATTERN = re.compile(r\"\\s+\", re.UNICODE)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Translates multiple whitespace into single space character.\n",
    "    If there is at least one new line character chunk is replaced\n",
    "    by single LF (Unix new line) character.\n",
    "    \"\"\"\n",
    "    return MULTIPLE_WHITESPACE_PATTERN.sub(_replace_whitespace, text)\n",
    "\n",
    "\n",
    "def _replace_whitespace(match):\n",
    "    text = match.group()\n",
    "\n",
    "    if \"\\n\" in text or \"\\r\" in text or \"\\r\\n\" in text:\n",
    "        return \"\\n\"\n",
    "    else:\n",
    "        return \" \"\n",
    "\n",
    "\n",
    "def is_blank(string):\n",
    "    \"\"\"\n",
    "    Returns `True` if string contains only white-space characters\n",
    "    or is empty. Otherwise `False` is returned.\n",
    "    \"\"\"\n",
    "    return not string or string.isspace()\n",
    "\n",
    "\n",
    "def get_symmetric_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Get Symmetric matrix\n",
    "    :param matrix:\n",
    "    :return: matrix\n",
    "    \"\"\"\n",
    "    return matrix + matrix.T - np.diag(matrix.diagonal())\n",
    "\n",
    "\n",
    "def core_cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    measure cosine similarity between two vectors\n",
    "    :param vector1:\n",
    "    :param vector2:\n",
    "    :return: 0 < cosine similarity value < 1\n",
    "    \"\"\"\n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def lemmatize_text(text):\n",
    "    lemmatized_text = \"\"\n",
    "    for sentence in text:\n",
    "        # tokenize the sentence and find the POS tag for each token\n",
    "        pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "\n",
    "        # we use our own pos_tagger function to make things simpler to understand.\n",
    "        wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in wordnet_tagged:\n",
    "            if tag is None:\n",
    "                # if there is no available tag, append the token as is\n",
    "                lemmatized_sentence.append(word)\n",
    "            else:\n",
    "                # else use the tag to lemmatize the token\n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "        lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "        lemmatized_text += lemmatized_sentence + \" \"\n",
    "    return lemmatized_text\n",
    "\n",
    "class TextRank4Sentences():\n",
    "    def __init__(self):\n",
    "        self.damping = 0.85  # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5  # convergence threshold\n",
    "        self.steps = 100  # iteration steps\n",
    "        self.text_str = None\n",
    "        self.sentences = None\n",
    "        self.pr_vector = None\n",
    "\n",
    "        # added for tf-idf\n",
    "        self.pr_vector2 = None\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "\n",
    "    def _sentence_similarity(self, sent1, sent2, stopwords=None):\n",
    "        if stopwords is None:\n",
    "            stopwords = []\n",
    "\n",
    "        sent1 = [w.lower() for w in sent1]\n",
    "        sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "        all_words = list(set(sent1 + sent2))\n",
    "\n",
    "        vector1 = [0] * len(all_words)\n",
    "        vector2 = [0] * len(all_words)\n",
    "        \n",
    "        #print(all_words)\n",
    "        # build the vector for the first sentence\n",
    "        for w in sent1:\n",
    "            if w in stopwords:\n",
    "                continue\n",
    "            vector1[all_words.index(w)] += 1\n",
    "\n",
    "        # build the vector for the second sentence\n",
    "        for w in sent2:\n",
    "            if w in stopwords:\n",
    "                continue\n",
    "            vector2[all_words.index(w)] += 1\n",
    "        \n",
    "        #print(vector1, vector2)\n",
    "        return core_cosine_similarity(vector1, vector2)\n",
    "\n",
    "    def _build_sent_graph(self, sentences):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentences).toarray()\n",
    "        \n",
    "        print(\"문장을 벡터화 시키기 위한 단어 집합:\\n\",self.tfidf.get_feature_names())\n",
    "        print()\n",
    "        print(\"TF-IDF 벡터화 된 문장:\\n\",tfidf_mat)\n",
    "        print()\n",
    "        #graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        sm = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "        for idx1 in range(len(sentences)):\n",
    "            for idx2 in range(len(sentences)):\n",
    "                if idx1 == idx2:\n",
    "                    continue\n",
    "                sm[idx1][idx2] = core_cosine_similarity(tfidf_mat[idx1], tfidf_mat[idx2])\n",
    "\n",
    "                \n",
    "        print(\"TF-IDF화 된 문장 간 코사인 유사도:\")\n",
    "        print('    ', end=\" \")\n",
    "        for i in range(len(sentences)):\n",
    "            print('  s%d  '%(i+1),end=\" \")\n",
    "        print()\n",
    "        for i in range(len(sentences)):\n",
    "            print('s%d'%(i+1), end=\" \")\n",
    "            for j in range(len(sentences)):\n",
    "                print('%.5f'%sm[i][j], end= \" \")\n",
    "            print()   \n",
    "        print()     \n",
    "        \n",
    "        sm = get_symmetric_matrix(sm)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(sm, axis=0)\n",
    "        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is to ignore the 0 element in norm\n",
    "\n",
    "        return sm_norm\n",
    "    \n",
    "    def _build_similarity_matrix(self, sentences, stopwords=None):\n",
    "        # create an empty similarity matrix\n",
    "        sm = np.zeros([len(sentences), len(sentences)])\n",
    "\n",
    "        for idx1 in range(len(sentences)):\n",
    "            for idx2 in range(len(sentences)):\n",
    "                if idx1 == idx2:\n",
    "                    continue\n",
    "\n",
    "                sm[idx1][idx2] = self._sentence_similarity(sentences[idx1], sentences[idx2], stopwords=stopwords)\n",
    "        \n",
    "        print(\"BoW 문장 간 코사인 유사도:\")\n",
    "        print('    ', end=\" \")\n",
    "        for i in range(len(sentences)):\n",
    "            print('  s%d  '%(i+1),end=\" \")\n",
    "        print()\n",
    "        for i in range(len(sentences)):\n",
    "            print('s%d'%(i+1), end=\" \")\n",
    "            for j in range(len(sentences)):\n",
    "                print('%.5f'%sm[i][j], end= \" \")\n",
    "            print()   \n",
    "        print()\n",
    "        # Get Symmeric matrix\n",
    "        sm = get_symmetric_matrix(sm)\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(sm, axis=0)\n",
    "        sm_norm = np.divide(sm, norm, where=norm != 0)  # this is to ignore the 0 element in norm\n",
    "        return sm_norm\n",
    "\n",
    "    def _run_page_rank(self, similarity_matrix):\n",
    "\n",
    "        pr_vector = np.array([1] * len(similarity_matrix))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr_vector = (1 - self.damping) + self.damping * np.matmul(similarity_matrix, pr_vector)\n",
    "            if abs(previous_pr - sum(pr_vector)) < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr_vector)\n",
    "        \n",
    "        return pr_vector\n",
    "\n",
    "    def _get_sentence(self, index):\n",
    "\n",
    "        try:\n",
    "            return self.sentences[index]\n",
    "        except IndexError:\n",
    "            return \"\"\n",
    "\n",
    "    def get_top_sentences(self, number=1):\n",
    "\n",
    "        top_sentences = {}\n",
    "\n",
    "        if self.pr_vector is not None:\n",
    "\n",
    "            sorted_pr = np.argsort(self.pr_vector)\n",
    "            sorted_pr = list(sorted_pr)\n",
    "            sorted_pr.reverse()\n",
    "\n",
    "            index = 0\n",
    "            for epoch in range(number):\n",
    "                # print(str(sorted_pr[index]) + \" : \" + str(self.pr_vector[sorted_pr[index]]))\n",
    "                sent = self.sentences[sorted_pr[index]]\n",
    "                sent = normalize_whitespace(sent)\n",
    "                top_sentences[index] = sent + \" : \" + str(self.pr_vector[sorted_pr[index]])\n",
    "                index += 1\n",
    "\n",
    "        return top_sentences\n",
    "\n",
    "    def get_top_sentences2(self, number=1):\n",
    "\n",
    "        top_sentences = {}\n",
    "\n",
    "        if self.pr_vector2 is not None:\n",
    "\n",
    "            sorted_pr = np.argsort(self.pr_vector2)\n",
    "            sorted_pr = list(sorted_pr)\n",
    "            sorted_pr.reverse()\n",
    "\n",
    "            index = 0\n",
    "            for epoch in range(number):\n",
    "                # print(str(sorted_pr[index]) + \" : \" + str(self.pr_vector[sorted_pr[index]]))\n",
    "                sent = self.sentences[sorted_pr[index]]\n",
    "                sent = normalize_whitespace(sent)\n",
    "                top_sentences[index] = sent + \" : \" + str(self.pr_vector2[sorted_pr[index]])\n",
    "                index += 1\n",
    "\n",
    "        return top_sentences\n",
    "\n",
    "    def analyze(self, text, stop_words=None, lem_flag=0):\n",
    "        self.text_str = text\n",
    "        self.sentences = sent_tokenize(self.text_str)\n",
    "\n",
    "        if lem_flag == 0:\n",
    "            temp = self.sentences\n",
    "        else:\n",
    "            temp = sent_tokenize(lemmatize_text(self.sentences))\n",
    "        # 특수문자 제거\n",
    "        eng_sentences = []\n",
    "        for sent in temp:\n",
    "            sent = re.sub('[^a-zA-Z]', ' ', sent)\n",
    "            eng_sentences.append(sent)\n",
    "        tokenized_sentences = [word_tokenize(sent) for sent in eng_sentences]\n",
    "\n",
    "        # stopwords를 제거한 tokenized sentences\n",
    "        rm_tokenized_sentences = []\n",
    "        for sent in tokenized_sentences:\n",
    "            temp = []\n",
    "            for word in sent:\n",
    "                if word not in stop_words:\n",
    "                    temp.append(word)\n",
    "            rm_tokenized_sentences.append(' '.join(temp))\n",
    "\n",
    "        similarity_matrix = self._build_similarity_matrix(tokenized_sentences, stop_words)\n",
    "        correlation_matrix = self._build_sent_graph(rm_tokenized_sentences)\n",
    "        \n",
    "        print(\"normalized sentence graph(cosine similarity):\\n\",similarity_matrix)\n",
    "        print()\n",
    "        print(\"normalized sentence graph(tf-idf + cosine similarity):\\n\",correlation_matrix)\n",
    "        self.pr_vector = self._run_page_rank(similarity_matrix)\n",
    "        self.pr_vector2 = self._run_page_rank(correlation_matrix)\n",
    "        print()\n",
    "        print(\"textrank vector(cosine similarity):\",self.pr_vector)\n",
    "        print()\n",
    "        print(\"textrank vector(tfidf + cosine):\",self.pr_vector2)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>문장에 대한 유사도를 구하기 위한 함수</h3>\n",
    "\n",
    "1. 인자로 리스트 형식의 토큰화 된 두개의 문장과 stopwords를 받아서 lower case로 문장 안의 단어들을 lower case로 만들어준다.\n",
    "2. 두 문장에 포함 된 모든 단어들을 집합 객체로 구성하여 집합 객체로 만들어 중복을 제거한다.\n",
    "3. 각 문장에 집합 객체에 포함된 단어가 몇개 들어 있는지 벡터로 나타내준다. stopwords에 포함되는 단어이면, continue하여 개수를 증가시키지 않는다.\n",
    "4. 벡터화 된 두 문장의 cosine similarity를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_sim(sent1, sent2, stopwords=None): # 토큰화 된 두개의 문장과 stopwords를 인자로 받음\n",
    "        if stopwords is None:\n",
    "            stopwords = []\n",
    "        \n",
    "        #문장을 모두 소문자로 바꿔줌\n",
    "        sent1 = [w.lower() for w in sent1]\n",
    "        sent2 = [w.lower() for w in sent2]\n",
    "            \n",
    "        all_words = list(set(sent1 + sent2))\n",
    "        \n",
    "        print(\"문장을 벡터화 시키기 위한 단어 집합:\", all_words)\n",
    "\n",
    "        vector1 = [0] * len(all_words)\n",
    "        vector2 = [0] * len(all_words)\n",
    "\n",
    "        # build the vector for the first sentence\n",
    "        for w in sent1:\n",
    "            if w in stopwords:\n",
    "                continue\n",
    "            vector1[all_words.index(w)] += 1\n",
    "\n",
    "        # build the vector for the second sentence\n",
    "        for w in sent2:\n",
    "            if w in stopwords:\n",
    "                continue\n",
    "            vector2[all_words.index(w)] += 1\n",
    "        \n",
    "        print(\"\\n1. Bow 형태로 벡터화 된 문장\")\n",
    "        print(\"벡터화된 문장 1:\",vector1)\n",
    "        print(\"벡터화된 문장 2:\", vector2)\n",
    "        \n",
    "        v = []\n",
    "        v.append(vector1)\n",
    "        v.append(vector2)\n",
    "        \n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. 문장을 BoW 형태로 벡터화</h3>\n",
    "\n",
    "문장 1: \"The Best Italian restaurant enjoy the best pasta.\"<br>\n",
    "문장 2: \"American restaurant enjoy the best hamburger.\"\n",
    "\n",
    "문장 간의 유사도를 계산하기 위해 전처리가 이루어진 문장의 벡터화가 제대로 이루어지는지 확인\n",
    "\n",
    "예상 결과)\n",
    "\n",
    "||restaurant|best|american|<span style=\"color:red\">the</span>|pasta|enjoy|italian|hamburger|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|s1|1|2|0|0|1|1|1|0|\n",
    "|s2|1|1|1|0|0|1|0|1|\n",
    "\n",
    "\n",
    "**'the'의 경우 nltk의 불용어로 값이 단어에 해당하는 값이 0이 됨**\n",
    "<hr/>\n",
    "<h3>2. tf - idf 형태로 벡터화</h3>\n",
    "<br>\n",
    "앞서 구했던 문장 벡터와 다음의 공식을 이용하여 tf - idf 벡터를 구해준다. <br>\n",
    "<h6>\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\">\n",
    "  <mtext>tf-idf(t,d)</mtext>\n",
    "  <mo>=</mo>\n",
    "  <mtext>tf(t,d)</mtext>\n",
    "  <mo>&#xD7;</mo>\n",
    "  <mtext>idf(t)</mtext>\n",
    "</math></h6><br>\n",
    "\n",
    "**tf란 문장에서 해당 단어가 얼마나 등장 했는지**<br>\n",
    "**idf란 (전체 문서수) / (해당 단어가 나타난 문서 수)**<br><br>\n",
    "scikit-learn의 경우 다음과 같은 idf 공식을 이용한다고 한다.<br>\n",
    "<h6>idf = log((1+n) / (1+df(t))) + 1 </h6><br>\n",
    "n: 전체 문서 수<br>\n",
    "df(t): t라는 용어를 포함하는 문서의 수<br>\n",
    "\n",
    "<hr/>\n",
    "<h3>3. 코사인 유사도 계산</h3>\n",
    "\n",
    "1) 직접 코사인 유사도 공식을 이용해 계산.<br>\n",
    "2) 이후 nltk의 cosine_distance를 이용해 비교. cosine_distance의 경우 1-cosine_similarity 값이므로, 1-cosine_distance를 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장을 벡터화 시키기 위한 단어 집합: ['hamburger', 'the', 'american', 'pasta', 'italian', 'restaurant', 'best', 'enjoy']\n",
      "\n",
      "1. Bow 형태로 벡터화 된 문장\n",
      "벡터화된 문장 1: [0, 0, 0, 1, 1, 1, 2, 1]\n",
      "벡터화된 문장 2: [1, 0, 1, 0, 0, 1, 1, 1]\n",
      "\n",
      "2. BoW를 이용한 코사인 유사도 계산\n",
      "1) 공식을 이용해 계산한 코사인 유사도 값: 0.6324555320336759\n",
      "2) nltk를 이용해 구한 코사인 유사도 값: 0.6324555320336759\n",
      "\n",
      "3. tf-idf 형태로 벡터화\n",
      "문장을 벡터화 시키기 위한 단어 집합: ['american', 'best', 'enjoy', 'hamburger', 'italian', 'pasta', 'restaurant']\n",
      "\n",
      "term-frequency vector 1: [0 2 1 0 1 1 1]\n",
      "term-frequency vector 2: [1 1 1 1 0 0 1]\n",
      "\n",
      "idf vector: [1.4054651081081644, 1.0, 1.0, 1.4054651081081644, 1.4054651081081644, 1.4054651081081644, 1.0]\n",
      "\n",
      "1) 직접 구한 tf-idf 값 1: [0.         0.63402146 0.31701073 0.         0.44554752 0.44554752\n",
      " 0.31701073]\n",
      "2) 직접 구한 tf-idf 값 2: [0.53309782 0.37930349 0.37930349 0.53309782 0.         0.\n",
      " 0.37930349]\n",
      "3) tfidf vectorizer 값 1: [0.         0.63402146 0.31701073 0.         0.44554752 0.44554752\n",
      " 0.31701073]\n",
      "4) tfidf vectorizer 값 2: [0.53309782 0.37930349 0.37930349 0.53309782 0.         0.\n",
      " 0.37930349]\n",
      "\n",
      "4. tf-idf를 이용한 코사인 유사도 계산\n",
      "1) 공식을 이용해 계산한 코사인 유사도 값: 0.48097310796014436\n",
      "2) nltk를 이용해 구한 코사인 유사도 값: 0.48097310796014436\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"The best Italian restaurant enjoy the best pasta.\"\n",
    "sentence2 = \"American restaurant enjoy the best hamburger.\"\n",
    "\n",
    "sentences = []\n",
    "sentences.append(sentence1)\n",
    "sentences.append(sentence2)\n",
    "\n",
    "sentence1 = re.sub('[^a-zA-Z]', ' ', sentence1)\n",
    "sentence2 = re.sub('[^a-zA-Z]', ' ', sentence2)\n",
    "\n",
    "sentence1 = word_tokenize(sentence1)\n",
    "sentence2 = word_tokenize(sentence2)\n",
    "\n",
    "v = sen_sim(sentence1, sentence2, stop_words)\n",
    "\n",
    "print(\"\\n2. BoW를 이용한 코사인 유사도 계산\")\n",
    "print(\"1) 공식을 이용해 계산한 코사인 유사도 값:\",np.dot(v[0],v[1])/(math.sqrt(np.dot(v[0],v[0])) * math.sqrt(np.dot(v[1],v[1]))))\n",
    "print(\"2) nltk를 이용해 구한 코사인 유사도 값:\", core_cosine_similarity(v[0],v[1]))\n",
    "\n",
    "print(\"\\n3. tf-idf 형태로 벡터화\")\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "tfidf_mat = tfidf.fit_transform(sentences).toarray()\n",
    "print(\"문장을 벡터화 시키기 위한 단어 집합:\",tfidf.get_feature_names())\n",
    "print()\n",
    "x = CountVectorizer(stop_words = 'english')\n",
    "c_mat =x.fit_transform(sentences).toarray()\n",
    "print(\"term-frequency vector 1:\",c_mat[0])\n",
    "print(\"term-frequency vector 2:\",c_mat[1])\n",
    "print()\n",
    "\n",
    "temp = [0] * len(c_mat[0])\n",
    "n = len(c_mat)\n",
    "\n",
    "for s in c_mat:\n",
    "    idx = 0\n",
    "    for w in s:\n",
    "        if w != 0:\n",
    "            temp[idx] += 1\n",
    "        idx+=1\n",
    "idf_vec = [0] * len(temp)\n",
    "idx = 0\n",
    "\n",
    "for i in temp:\n",
    "    idf_vec[idx] = math.log((n+1)/(i+1)) + 1\n",
    "    idx += 1\n",
    "    \n",
    "print(\"idf vector:\",idf_vec)\n",
    "print()\n",
    "\n",
    "temp = np.zeros((2,7))\n",
    "\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, len(idf_vec)):\n",
    "        temp[i][j] = (c_mat[i][j] * idf_vec[j])\n",
    "\n",
    "norm1 = math.sqrt(np.dot(temp[0],temp[0]))\n",
    "norm2 = math.sqrt(np.dot(temp[1],temp[1]))\n",
    "\n",
    "for i in range(0, 7):\n",
    "    temp[0][i] = temp[0][i] / norm1\n",
    "for i in range(0, 7):\n",
    "    temp[1][i] = temp[1][i] / norm2\n",
    "\n",
    "print(\"1) 직접 구한 tf-idf 값 1:\",temp[0])\n",
    "print(\"2) 직접 구한 tf-idf 값 2:\",temp[1])\n",
    "\n",
    "print(\"3) tfidf vectorizer 값 1:\",tfidf_mat[0])\n",
    "print(\"4) tfidf vectorizer 값 2:\",tfidf_mat[1])\n",
    "\n",
    "print(\"\\n4. tf-idf를 이용한 코사인 유사도 계산\")\n",
    "print(\"1) 공식을 이용해 계산한 코사인 유사도 값:\",np.dot(tfidf_mat[0],tfidf_mat[1])/(math.sqrt(np.dot(tfidf_mat[0],tfidf_mat[0])) * math.sqrt(np.dot(tfidf_mat[1],tfidf_mat[1]))))\n",
    "print(\"2) nltk를 이용해 구한 코사인 유사도 값:\", core_cosine_similarity(tfidf_mat[0],tfidf_mat[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Sentence Graph와 PageRank를 이용하여 핵심 문장 추출하기</h2>\n",
    "\n",
    "<h3>벡터화 된 문장간의 코사인 유사도를 이용하여 문장 그래프 생성</h3>\n",
    "\n",
    "![Cormat](Downloads/cormat.jpeg)\n",
    "**다음과 같은 코드를 이용해서 세 문장을 요약해본다** \n",
    "\n",
    "> <h3>1) BoW 문장 그래프 생성</h3>\n",
    ">\n",
    ">![similarity graph](Downloads/a.png)\n",
    ">\n",
    "> 1. 문장간의 유사도를 행렬로 나타내기 위해, 문장의 갯수 * 문장의 갯수 크기의 행렬을 생성한다.\n",
    "> 2. 앞서 구현한 sentence similarity 함수를 이용하여 문장간의 sentence similarity를 그래프로 나타낸다.<br> \n",
    "> ex) <br> \n",
    "> s1: I could not solve many problems in today's test.<br>\n",
    "> s2: My friends said the test was hard too.<br>\n",
    "> s3: The test was very hard.\n",
    "> <br>\n",
    "> <br>\n",
    "> s1과 s2를 벡터화 시키면 아래와 같이 나온다. <br>\n",
    "> \n",
    "> ||i|not|problems|the|hard|could|said|my|was|friends|too|today|test|in|many|solve|s|\n",
    "> |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "> |s1|0|0|1|0|0|1|0|0|0|0|0|1|1|0|1|1|0|\n",
    "> |s2|0|0|0|0|1|0|1|0|0|1|0|0|1|0|0|0|0|\n",
    ">  \n",
    ">  <br>\n",
    ">\n",
    "> 코사인 유사도 공식을 이용하여 두 문장의 코사인 유사도를 구하면 0.204124가 나온다. 코드로도 같은 값이 나오는지 확인해본다.<br>\n",
    "> 3. 구한 코사인 유사도 행렬을 열에 대해 normalize 한(합을 1로 바꿔준) 그래프로 pagerank 알고리즘을 수행시킨다.<br>\n",
    "> 4. 생각한 핵심 문장(s3)이 제대로 추출되었는지 확인한다.\n",
    ">\n",
    "> <h3> 2) TF-IDF 문장 그래프 생성 </h3>\n",
    ">\n",
    ">![tf_idf_similarity_graph](Downloads/b.png)\n",
    "> <br> 문장을 벡터화 시키는 방법만 TF-IDF로 바꿔 2)와 동일한 과정을 수행한다.<br>\n",
    "> 사용되는 단어 집합은 (could, friends, hard, many, my, probelms, said, solve, test, the today)\n",
    "> TF-IDF값을 엑셀로 계산하여 제대로 나오는지 확인\n",
    "> ![c](Downloads/c.jpeg)\n",
    ">\n",
    "> <h3> 3) PageRank 알고리즘을 이용한 핵심 문장 추출 </h3>\n",
    "> <br> 앞서 생성한 normalize된 문장 그래프를 pagerank 알고리즘에 대입하여 textrank vector를 생성하여, 가장 높은 값을 가지는 인덱스에 해당하는 문장을 추출한다.\n",
    ">\n",
    ">![pagerank_alg](Downloads/d.png)\n",
    "> 1. pagerank vector는 각각의 문장의 중요도에 해당하는 값을 1로 초기화 한 벡터를 앞서 구한 행렬과 damping factor(0.85)를 곱하여 계산한다.\n",
    "> 2. 새로 구한 textrank vector와 계산 전의 textrank vector의 차가 min diff(1e-5) 보다 작거나, 계산을 100회 수행하였다면 멈춘다.\n",
    "> \n",
    "> ![pagerank_alg](Downloads/e.png)\n",
    "> 3. 계산 과정을 간략하게 따라해보면 새로 계산되는 textrank vector는 해당 sentence와 연결된 문장의 유사성과 weight에 의해 결정된다. <br>\n",
    "> ex) 그림에서 볼 수 있듯이 s1의 weight은 s1과 s2사이의 유사성에 s2의 weight을 곱하고, s1과 s3사이의 유사성에 s3의 weight을 곱해 더해준 값이다. <br> <br>\n",
    "> _즉 연결된 문장에 유사성이 큰 문장이 많을수록 더 중요한 문장이 되고, 중요한 문장들이 높은 유사성으로 연결되어 있을때, 그 문장이 핵심 문장이 된다._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW 문장 간 코사인 유사도:\n",
      "       s1     s2     s3   \n",
      "s1 0.00000 0.20412 0.28868 \n",
      "s2 0.20412 0.00000 0.70711 \n",
      "s3 0.28868 0.70711 0.00000 \n",
      "\n",
      "문장을 벡터화 시키기 위한 단어 집합:\n",
      " ['could', 'friends', 'hard', 'many', 'my', 'problems', 'said', 'solve', 'test', 'the', 'today']\n",
      "\n",
      "TF-IDF 벡터화 된 문장:\n",
      " [[0.43238509 0.         0.         0.43238509 0.         0.43238509\n",
      "  0.         0.43238509 0.2553736  0.         0.43238509]\n",
      " [0.         0.50461134 0.38376993 0.         0.50461134 0.\n",
      "  0.50461134 0.         0.29803159 0.         0.        ]\n",
      " [0.         0.         0.54783215 0.         0.         0.\n",
      "  0.         0.         0.42544054 0.72033345 0.        ]]\n",
      "\n",
      "TF-IDF화 된 문장 간 코사인 유사도:\n",
      "       s1     s2     s3   \n",
      "s1 0.00000 0.07611 0.10865 \n",
      "s2 0.07611 0.00000 0.33704 \n",
      "s3 0.10865 0.33704 0.00000 \n",
      "\n",
      "normalized sentence graph(cosine similarity):\n",
      " [[0.         0.22400924 0.28989795]\n",
      " [0.41421356 0.         0.71010205]\n",
      " [0.58578644 0.77599076 0.        ]]\n",
      "\n",
      "normalized sentence graph(tf-idf + cosine similarity):\n",
      " [[0.         0.1842193  0.24377506]\n",
      " [0.41194619 0.         0.75622494]\n",
      " [0.58805381 0.8157807  0.        ]]\n",
      "\n",
      "textrank vector(cosine similarity): [0.68271587 1.14580495 1.17147918]\n",
      "\n",
      "textrank vector(tfidf + cosine): [0.60730488 1.19334286 1.19935226]\n",
      "\n",
      "BoW 방식을 이용한 핵심 문장\n",
      "The test was very hard. : 1.1714791798458153\n",
      "\n",
      "TF-IDF 방식을 이용한 핵심 문장\n",
      "The test was very hard. : 1.1993522634548994\n"
     ]
    }
   ],
   "source": [
    "test_str = \"\"\"I could not solve many problems in today's test. \n",
    "My friends said the test was hard too. \n",
    "The test was very hard.\"\"\"\n",
    "\n",
    "\n",
    "tr4sh = TextRank4Sentences()\n",
    "tr4sh.analyze(test_str, stop_words)\n",
    "top_sentence = tr4sh.get_top_sentences(1)\n",
    "\n",
    "print(\"BoW 방식을 이용한 핵심 문장\")\n",
    "print(top_sentence[0])\n",
    "print()\n",
    "top_sentence_tfidf = tr4sh.get_top_sentences2(1)\n",
    "print(\"TF-IDF 방식을 이용한 핵심 문장\")\n",
    "print(top_sentence_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Term Frequency 방식의 한계 </h3>\n",
    "\n",
    "s1: She looks really really pretty.<br>\n",
    "s2: She is really really kind. <br>\n",
    "s3: Really Really Really. <br>\n",
    "s4: Sincerely Jane is so nice. <br>\n",
    "s5: That is why I really love her. <br> \n",
    "\n",
    "1. 유사한 의미의 단어를 구별하지 못함 <br>\n",
    "s2와 s4의 의미적으로 동일한 문장이지만, 문장간의 유사성이 없다고 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW 문장 간 코사인 유사도:\n",
      "       s1     s2     s3     s4     s5   \n",
      "s1 0.00000 0.73030 0.81650 0.00000 0.57735 \n",
      "s2 0.73030 0.00000 0.89443 0.00000 0.63246 \n",
      "s3 0.81650 0.89443 0.00000 0.00000 0.70711 \n",
      "s4 0.00000 0.00000 0.00000 0.00000 0.00000 \n",
      "s5 0.57735 0.63246 0.70711 0.00000 0.00000 \n",
      "\n",
      "문장을 벡터화 시키기 위한 단어 집합:\n",
      " ['jane', 'kind', 'looks', 'love', 'nice', 'pretty', 'really', 'she', 'sincerely', 'that']\n",
      "\n",
      "TF-IDF 벡터화 된 문장:\n",
      " [[0.         0.         0.50504305 0.         0.         0.50504305\n",
      "  0.56906489 0.40746555 0.         0.        ]\n",
      " [0.         0.58515407 0.         0.         0.         0.\n",
      "  0.65933119 0.47209862 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.         0.         0.57735027 0.        ]\n",
      " [0.         0.         0.         0.65690037 0.         0.\n",
      "  0.37008621 0.         0.         0.65690037]]\n",
      "\n",
      "TF-IDF화 된 문장 간 코사인 유사도:\n",
      "       s1     s2     s3     s4     s5   \n",
      "s1 0.00000 0.56757 0.56906 0.00000 0.21060 \n",
      "s2 0.56757 0.00000 0.65933 0.00000 0.24401 \n",
      "s3 0.56906 0.65933 0.00000 0.00000 0.37009 \n",
      "s4 0.00000 0.00000 0.00000 0.00000 0.00000 \n",
      "s5 0.21060 0.24401 0.37009 0.00000 0.00000 \n",
      "\n",
      "normalized sentence graph(cosine similarity):\n",
      " [[0.         0.32354394 0.33767008 0.         0.30118758]\n",
      " [0.34380762 0.         0.36989904 0.         0.32993447]\n",
      " [0.3843886  0.39625878 0.         0.         0.36887795]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.27180379 0.28019727 0.29243087 0.         0.        ]]\n",
      "\n",
      "normalized sentence graph(tf-idf + cosine similarity):\n",
      " [[0.         0.38586142 0.35600325 0.         0.25536972]\n",
      " [0.4212825  0.         0.41247325 0.         0.29587702]\n",
      " [0.42239495 0.44824813 0.         0.         0.44875326]\n",
      " [0.         0.         0.         0.         0.        ]\n",
      " [0.15632255 0.16589045 0.2315235  0.         0.        ]]\n",
      "\n",
      "textrank vector(cosine similarity): [0.98086927 1.03063248 1.08768788 0.15       0.90081036]\n",
      "\n",
      "textrank vector(tfidf + cosine): [1.03360389 1.10913173 1.16793621 0.15       0.68932817]\n",
      "\n",
      "BoW 방식을 이용한 핵심 문장\n",
      "Really Really Really. : 1.0876878835188721\n",
      "\n",
      "TF-IDF 방식을 이용한 핵심 문장\n",
      "Really Really Really. : 1.167936214807265\n"
     ]
    }
   ],
   "source": [
    "test_str = \"\"\"She looks really really pretty. \n",
    "She is really really kind. \n",
    "Really Really Really.\n",
    "Sincerely Jane is so nice.\n",
    "That is why I really love her.\"\"\"\n",
    "\n",
    "\n",
    "tr4sh = TextRank4Sentences()\n",
    "tr4sh.analyze(test_str, stop_words)\n",
    "top_sentence = tr4sh.get_top_sentences(1)\n",
    "\n",
    "print(\"BoW 방식을 이용한 핵심 문장\")\n",
    "print(top_sentence[0])\n",
    "print()\n",
    "top_sentence_tfidf = tr4sh.get_top_sentences2(1)\n",
    "print(\"TF-IDF 방식을 이용한 핵심 문장\")\n",
    "print(top_sentence_tfidf[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
